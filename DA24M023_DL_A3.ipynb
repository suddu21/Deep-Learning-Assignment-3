{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11765869,"sourceType":"datasetVersion","datasetId":7386471}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DA6401 Assignment 3 - Transliteration model (Encoder/Decoder model)\n## Sudhanva Satish - DA24M023","metadata":{}},{"cell_type":"code","source":"# 1. Imports & Setup\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, Dense, SimpleRNN, LSTM, GRU\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.optimizers import Adam\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n\nwandb.login(key=\"<Add your key>\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Load Dakshina Dataset\nDATA_DIR = \"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/hi/lexicons\"\n\ndef load_tsv(path):\n    with open(path, encoding='utf-8') as f:\n        lines = f.read().strip().split('\\n')\n    return [line.split('\\t') for line in lines if '\\t' in line]\n\ntrain_pairs = load_tsv(os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\"))\nval_pairs = load_tsv(os.path.join(DATA_DIR, \"hi.translit.sampled.dev.tsv\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Load Dakshina Dataset\nDATA_DIR = \"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/ka/lexicons\"\n\ndef load_tsv(path):\n    with open(path, encoding='utf-8') as f:\n        lines = f.read().strip().split('\\n')\n    return [line.split('\\t') for line in lines if '\\t' in line]\n\ntrain_pairs = load_tsv(os.path.join(DATA_DIR, \"ka.translit.sampled.train.tsv\"))\nval_pairs = load_tsv(os.path.join(DATA_DIR, \"ka.translit.sampled.dev.tsv\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Tokenization\ndef tokenize_pairs(pairs):\n    latin_texts = [x[1] for x in pairs]         # What user types (input)\n    devanagari_texts = [x[0] for x in pairs]    # What model should output\n\n    devanagari_texts_in  = ['\\t' + t for t in devanagari_texts]\n    devanagari_texts_out = [t + '\\n' for t in devanagari_texts]\n    return latin_texts, devanagari_texts_in, devanagari_texts_out\n\n\ntrain_lat, train_deva_in, train_deva_out = tokenize_pairs(train_pairs)\nval_lat, val_deva_in, val_deva_out = tokenize_pairs(val_pairs)\n\ndef fit_char_tokenizer(texts):\n    tokenizer = Tokenizer(char_level=True, lower=False)\n    tokenizer.fit_on_texts(texts)\n    return tokenizer\n\ninput_tokenizer = fit_char_tokenizer(train_lat + val_lat)\ntarget_tokenizer = fit_char_tokenizer(train_deva_in + train_deva_out)\n\nVOCAB_SIZE_INPUT = len(input_tokenizer.word_index) + 1\nVOCAB_SIZE_TARGET = len(target_tokenizer.word_index) + 1\n\ndef encode_and_pad(texts, tokenizer, maxlen=None):\n    return pad_sequences(tokenizer.texts_to_sequences(texts), padding='post', maxlen=maxlen)\n\nMAXLEN_INPUT = max(map(len, train_lat))\nMAXLEN_TARGET = max(map(len, train_deva_out))\n\ntrain_encoder_input = encode_and_pad(train_lat, input_tokenizer, MAXLEN_INPUT)\ntrain_decoder_input = encode_and_pad(train_deva_in, target_tokenizer, MAXLEN_TARGET)\ntrain_target_output = np.expand_dims(encode_and_pad(train_deva_out, target_tokenizer, MAXLEN_TARGET), -1)\n\nval_encoder_input = encode_and_pad(val_lat, input_tokenizer, MAXLEN_INPUT)\nval_decoder_input = encode_and_pad(val_deva_in, target_tokenizer, MAXLEN_TARGET)\nval_target_output = np.expand_dims(encode_and_pad(val_deva_out, target_tokenizer, MAXLEN_TARGET), -1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_lat[:5])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Error checking\n\"\"\"start_token = target_tokenizer.word_index.get('<s>')\nend_token = target_tokenizer.word_index.get('</s>')\n\nif start_token is None or end_token is None:\n    raise ValueError(\"Start/end tokens not found in tokenizer vocabulary. Did you include <s> and </s> during training?\")\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question 1 - Build the model","metadata":{}},{"cell_type":"code","source":"# 4. Build Seq2Seq Model\ndef build_seq2seq_model(vocab_size, embedding_dim, hidden_dim, cell_type, num_encoder_layers, num_decoder_layers, dropout_rate):\n    encoder_inputs = Input(shape=(None,))\n    decoder_inputs = Input(shape=(None,))\n    embedding = Embedding(vocab_size, embedding_dim, mask_zero=True, name=\"embedding\")\n    enc_emb = embedding(encoder_inputs)\n    dec_emb = embedding(decoder_inputs)\n    RNN = {\"RNN\": SimpleRNN, \"LSTM\": LSTM}[cell_type]\n\n    x = enc_emb\n    for i in range(num_encoder_layers):\n        rnn = RNN(hidden_dim, return_sequences=True, return_state=True, dropout=dropout_rate, name=f\"encoder_{cell_type}_{i}\")\n        if cell_type == \"LSTM\":\n            x, state_h, state_c = rnn(x)\n            encoder_states = [state_h, state_c]\n        else:\n            x, state_h = rnn(x)\n            encoder_states = [state_h]\n\n    y = dec_emb\n    for i in range(num_decoder_layers):\n        rnn = RNN(hidden_dim, return_sequences=True, return_state=True, dropout=dropout_rate, name=f\"decoder_{cell_type}_{i}\")\n        if cell_type == \"LSTM\":\n            y, _, _ = rnn(y, initial_state=encoder_states)\n        else:\n            y, _ = rnn(y, initial_state=encoder_states)\n\n    decoder_dense = Dense(vocab_size, activation=\"softmax\", name=\"dense\")\n    outputs = decoder_dense(y)\n    return Model([encoder_inputs, decoder_inputs], outputs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Inference Models (LSTM only)\ndef build_inference_models(model, hidden_dim, cell_type):\n    encoder_inputs = model.input[0]\n    decoder_inputs = model.input[1]\n    embedding_layer = model.get_layer(\"embedding\")\n    encoder_emb = embedding_layer(encoder_inputs)\n    decoder_emb = embedding_layer(decoder_inputs)\n\n    # Dynamically get the first matching RNN layer for encoder and decoder\n    encoder_rnn = next(layer for layer in model.layers if layer.name.startswith(f\"encoder_{cell_type}\"))\n    decoder_rnn = next(layer for layer in model.layers if layer.name.startswith(f\"decoder_{cell_type}\"))\n    decoder_dense = model.get_layer(\"dense\")\n\n    if cell_type == \"LSTM\":\n        _, state_h, state_c = encoder_rnn(encoder_emb)\n        encoder_model = Model(encoder_inputs, [state_h, state_c])\n\n        decoder_state_input_h = Input(shape=(hidden_dim,))\n        decoder_state_input_c = Input(shape=(hidden_dim,))\n        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n        decoder_outputs, state_h, state_c = decoder_rnn(decoder_emb, initial_state=decoder_states_inputs)\n        decoder_outputs = decoder_dense(decoder_outputs)\n\n        decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs, state_h, state_c])\n    else:  # RNN\n        _, state_h = encoder_rnn(encoder_emb)\n        encoder_model = Model(encoder_inputs, [state_h])\n\n        decoder_state_input_h = Input(shape=(hidden_dim,))\n        decoder_outputs, state_h = decoder_rnn(decoder_emb, initial_state=[decoder_state_input_h])\n        decoder_outputs = decoder_dense(decoder_outputs)\n\n        decoder_model = Model([decoder_inputs, decoder_state_input_h], [decoder_outputs, state_h])\n\n    return encoder_model, decoder_model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Beam Search Decoder\ndef decode_sequence_beam_search(input_seq, encoder_model, decoder_model, target_tokenizer, beam_width=3, max_output_len=30):\n    index_to_char = {i: c for c, i in target_tokenizer.word_index.items()}\n    index_to_char[0] = ''\n    start_token = target_tokenizer.word_index['<s>']\n    end_token = target_tokenizer.word_index['</s>']\n\n    states_value = encoder_model.predict(input_seq)\n    sequences = [([start_token], 0.0, states_value)]\n\n    for _ in range(max_output_len):\n        all_candidates = []\n        for seq, score, states in sequences:\n            if seq[-1] == end_token:\n                all_candidates.append((seq, score, states))\n                continue\n            target_seq = np.array([[seq[-1]]])\n            output_tokens, h, c = decoder_model.predict([target_seq] + states)\n            top_k = np.argsort(output_tokens[0, -1, :])[-beam_width:]\n\n            for token in top_k:\n                prob = output_tokens[0, -1, token]\n                candidate = (seq + [token], score - np.log(prob + 1e-9), [h, c])\n                all_candidates.append(candidate)\n\n        sequences = sorted(all_candidates, key=lambda tup: tup[1])[:beam_width]\n\n    best_seq = sequences[0][0]\n    return ''.join(index_to_char.get(idx, '') for idx in best_seq[1:] if idx != end_token)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question 2 - Train the model via wandb sweeps","metadata":{}},{"cell_type":"code","source":"# 7. WandB Sweep Config\nsweep_config = {\n    'method': 'bayes',\n    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'embedding_dim': {'values': [16, 32, 64, 256]},\n        'hidden_dim': {'values': [16, 32, 64, 256]},\n        'cell_type': {'values': ['RNN', 'LSTM']},\n        'num_encoder_layers': {'values': [1, 2]},\n        'num_decoder_layers': {'values': [1, 2]},\n        'dropout_rate': {'values': [0.2, 0.3]},\n        'batch_size': {'values': [32, 64]},\n        'epochs': {'value': 10},\n        'beam_size': {'values': [1, 3, 5]}\n    }\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 8. Sweep Train Function\ndef sweep_train(config=None):\n    with wandb.init(config=config):\n        config = wandb.config\n        wandb.run.name = f\"{config.cell_type}_emb{config.embedding_dim}_hid{config.hidden_dim}_enc{config.num_encoder_layers}_dec{config.num_decoder_layers}_drop{int(config.dropout_rate*100)}_beam{config.beam_size}\"\n\n        model = build_seq2seq_model(\n            vocab_size=VOCAB_SIZE_TARGET,\n            embedding_dim=config.embedding_dim,\n            hidden_dim=config.hidden_dim,\n            cell_type=config.cell_type,\n            num_encoder_layers=config.num_encoder_layers,\n            num_decoder_layers=config.num_decoder_layers,\n            dropout_rate=config.dropout_rate\n        )\n\n        model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n        model.fit(\n            [train_encoder_input, train_decoder_input],\n            train_target_output,\n            validation_data=([val_encoder_input, val_decoder_input], val_target_output),\n            batch_size=config.batch_size,\n            epochs=config.epochs,\n            callbacks=[WandbMetricsLogger(), WandbModelCheckpoint(\"model_checkpoint.keras\")],\n            verbose=2\n        )\n\n        # Beam Search part - commented to reduce runtime\n        \"\"\"encoder_model, decoder_model = build_inference_models(model, config.hidden_dim, config.cell_type)\n        input_seq = val_encoder_input[0:1]\n        prediction = decode_sequence_beam_search(\n            input_seq=input_seq,\n            encoder_model=encoder_model,\n            decoder_model=decoder_model,\n            target_tokenizer=target_tokenizer,\n            beam_width=config.beam_size\n        )\n\n        print(\"Predicted:\", prediction)\n        print(\"Ground truth:\", val_deva_out[0])\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 9. Start the Sweep\nsweep_id = wandb.sweep(sweep_config, project=\"DL_A3\")\nwandb.agent(sweep_id, function=sweep_train, count=100)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question 3 in report","metadata":{}},{"cell_type":"markdown","source":"# Question 4 - Evaluate model on Test set","metadata":{}},{"cell_type":"code","source":"best_config = {\n    'embedding_dim': 256,\n    'hidden_dim': 256,\n    'cell_type': 'LSTM',\n    'num_encoder_layers': 2,\n    'num_decoder_layers': 2,\n    'dropout_rate': 0.2,\n    'batch_size': 64,\n    'epochs': 10#20\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model = build_seq2seq_model(\n    vocab_size=VOCAB_SIZE_TARGET,\n    embedding_dim=best_config['embedding_dim'],\n    hidden_dim=best_config['hidden_dim'],\n    cell_type=best_config['cell_type'],\n    num_encoder_layers=best_config['num_encoder_layers'],\n    num_decoder_layers=best_config['num_decoder_layers'],\n    dropout_rate=best_config['dropout_rate']\n)\n\nbest_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nbest_model.fit(\n    [train_encoder_input, train_decoder_input],\n    train_target_output,\n    validation_data=([val_encoder_input, val_decoder_input], val_target_output),\n    batch_size=best_config['batch_size'],\n    epochs=best_config['epochs'],\n    verbose=2\n)\n\nbest_model.save(\"best_model.keras\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nbest_model = load_model(\"best_model.keras\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test data\ntest_path = \"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n#test_path = \"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/ka/lexicons/ka.translit.sampled.test.tsv\"\ntest_pairs = load_tsv(test_path)\ntest_lat, test_deva_in, test_deva_out = tokenize_pairs(test_pairs)\n\ntest_encoder_input = encode_and_pad(test_lat, input_tokenizer, MAXLEN_INPUT)\ntest_decoder_input = encode_and_pad(test_deva_in, target_tokenizer, MAXLEN_TARGET)\ntest_target_output = encode_and_pad(test_deva_out, target_tokenizer, MAXLEN_TARGET)\ntest_target_output = np.expand_dims(test_target_output, -1)\n\nwandb.init(project=\"DL_A3\", name=\"Vanilla_best_test\")\n\ntest_loss, test_acc = best_model.evaluate([test_encoder_input, test_decoder_input], test_target_output)\nprint(f\"✅ Test Accuracy: {test_acc:.4f}\")\n\nwandb.log({'test_accuracy':test_acc})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index_to_char = {i: c for c, i in target_tokenizer.word_index.items()}\nindex_to_char[0] = ''\n\ndef decode_seq(seq):\n    decoded = []\n    for idx in seq:\n        if idx == 0:\n            continue\n        token = index_to_char.get(idx, '')  # fallback to '' if invalid index\n        # check this token based on what was given first\n        if token in ['\\n']:\n            break\n        decoded.append(token)\n    return ''.join(decoded)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(\"predictions_vanilla\", exist_ok=True)\npreds = best_model.predict([test_encoder_input, test_decoder_input])\npred_indices = np.argmax(preds, axis=-1)\n\nindex_to_char = {i: c for c, i in target_tokenizer.word_index.items()}\nindex_to_char[0] = ''\n\ndef decode_seq(seq):\n    decoded = []\n    for idx in seq:\n        if idx == 0:\n            continue\n        token = index_to_char.get(idx, '')  # fallback to '' if invalid index\n        if token in ['\\n']:#['<','\\\\','/']:\n            break\n        decoded.append(token)\n    return ''.join(decoded)\n\n\ndecoded_preds = [decode_seq(seq) for seq in pred_indices]\n# Here too\ndecoded_refs = [x.replace(' </s>', '') for x in test_deva_out]\n\nwith open(\"predictions_vanilla/test_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n    for inp, pred, ref in zip(test_lat, decoded_preds, decoded_refs):\n        f.write(f\"{inp}\\t{pred}\\t{ref}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display first 10 test predictions\nfor i in range(10):\n    input_latin = test_lat[i]\n    predicted = decoded_preds[i]\n    reference = decoded_refs[i]\n    print(f\"{i+1}. Input: {input_latin}\\n   Predicted: {predicted}\\n   Reference: {reference}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\n\n# Initialize a Wandb run\nwandb.init(project=\"seq2seq_sweep\", name=\"prediction_samples_colored_table_10\")\n\n# Create a Wandb table\ntable = wandb.Table(columns=[\"Input Word\", \"Predicted Word\", \"Target Word\"])\n\n# Sample predictions (10 rows)\nsamples = [\n    (\"ankit\", \"अंकीत\", \"अंकित\"),\n    (\"angreji\", \"अगग्ेजी\", \"अंग्रज़ी\"),\n    (\"andhapan\", \"अंधापन\", \"अंधापन\"),\n    (\"achnera\", \"अच्रेर\", \"अछनेरा\"),\n    (\"advait\", \"एडववि\", \"अद्वैत\"),\n    (\"aakar\", \"आकार\", \"आकार\"),\n    (\"anupam\", \"अनुपम\", \"अनुपम\"),\n    (\"aadesh\", \"आदेश\", \"आदेश\"),\n    (\"abhay\", \"अभाय\", \"अभय\"),\n    (\"aastik\", \"आस्तिक्\", \"आस्तिक\")\n]\n\n# Add data with color-coded Predicted Word\nfor input_word, pred_word, target_word in samples:\n    # Determine color based on exact match\n    color = \"#00FF00\" if pred_word == target_word else \"#FF0000\"\n    # Wrap predicted word in HTML span\n    colored_pred = f'<span style=\"color: {color}\">{pred_word}</span>'\n    table.add_data(input_word, colored_pred, target_word)\n\n# Log the table to the Wandb run\nwandb.log({\"Prediction Samples Colored Table\": table})\n\n# Finish the run\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question 5 - Add Attention mechanism to model","metadata":{}},{"cell_type":"code","source":"attention_config = {\n    'embedding_dim': 256,\n    'hidden_dim': 256,\n    'dropout_rate': 0.2,\n    'batch_size': 64,\n    'epochs': 10\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Attention, Concatenate\n\nclass BahdanauAttention(Layer):\n    def __init__(self, units):\n        super().__init__()\n        self.W1 = Dense(units)\n        self.W2 = Dense(units)\n        self.V = Dense(1)\n\n    def call(self, query, values):\n        # query: (batch_size, dec_len, hidden)\n        # values: (batch_size, enc_len, hidden)\n        query_with_time_axis = tf.expand_dims(query, 2)  # (batch_size, dec_len, 1, hidden)\n        values_with_time_axis = tf.expand_dims(values, 1)  # (batch_size, 1, enc_len, hidden)\n\n        score = self.V(tf.nn.tanh(self.W1(values_with_time_axis) + self.W2(query_with_time_axis)))  # (batch_size, dec_len, enc_len, 1)\n        attention_weights = tf.nn.softmax(score, axis=2)  # (batch_size, dec_len, enc_len, 1)\n        context_vector = attention_weights * values_with_time_axis  # (batch_size, dec_len, enc_len, hidden)\n        context_vector = tf.reduce_sum(context_vector, axis=2)  # (batch_size, dec_len, hidden)\n        return context_vector, tf.squeeze(attention_weights, -1)  # return both context and weights\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Modified attention class to export weights\n\nclass BahdanauAttention(Layer):\n    def __init__(self, units, return_attention=False):\n        super().__init__()\n        self.W1 = Dense(units)\n        self.W2 = Dense(units)\n        self.V = Dense(1)\n        self.return_attention = return_attention\n\n    def call(self, query, values):\n        query_with_time_axis = tf.expand_dims(query, 2)  # (batch, dec_len, 1, hidden)\n        values_with_time_axis = tf.expand_dims(values, 1)  # (batch, 1, enc_len, hidden)\n\n        score = self.V(tf.nn.tanh(self.W1(values_with_time_axis) + self.W2(query_with_time_axis)))  # (batch, dec_len, enc_len, 1)\n        attention_weights = tf.nn.softmax(score, axis=2)\n        context_vector = tf.reduce_sum(attention_weights * values_with_time_axis, axis=2)\n\n        if self.return_attention:\n            return context_vector, tf.squeeze(attention_weights, -1)\n        return context_vector\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_attention_seq2seq_model(vocab_size_input, vocab_size_target, embedding_dim, hidden_dim, dropout_rate):\n    # Encoder\n    encoder_inputs = Input(shape=(None,))\n    encoder_emb = Embedding(vocab_size_input, embedding_dim, mask_zero=True)(encoder_inputs)\n    encoder_outputs, state_h, state_c = LSTM(hidden_dim, return_sequences=True, return_state=True)(encoder_emb)\n\n    # Decoder\n    decoder_inputs = Input(shape=(None,))\n    decoder_emb = Embedding(vocab_size_target, embedding_dim, mask_zero=True)(decoder_inputs)\n    decoder_outputs, _, _ = LSTM(hidden_dim, return_sequences=True, return_state=True)(decoder_emb, initial_state=[state_h, state_c])\n\n    # Attention\n    context_vector, attention_weights = BahdanauAttention(hidden_dim)(decoder_outputs, encoder_outputs)\n    concat = Concatenate()([decoder_outputs, context_vector])\n\n    outputs = Dense(vocab_size_target, activation='softmax')(concat)\n\n    model = Model([encoder_inputs, decoder_inputs], outputs)\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Attention extraction model\n\ndef build_attention_seq2seq_model(vocab_size_input, vocab_size_target, embedding_dim, hidden_dim, dropout_rate):\n    # Encoder\n    encoder_inputs = Input(shape=(None,))\n    encoder_emb = Embedding(vocab_size_input, embedding_dim, mask_zero=True)(encoder_inputs)\n    encoder_outputs, state_h, state_c = LSTM(hidden_dim, return_sequences=True, return_state=True)(encoder_emb)\n\n    # Decoder\n    decoder_inputs = Input(shape=(None,))\n    decoder_emb = Embedding(vocab_size_target, embedding_dim, mask_zero=True)(decoder_inputs)\n    decoder_outputs, _, _ = LSTM(hidden_dim, return_sequences=True, return_state=True)(decoder_emb, initial_state=[state_h, state_c])\n\n    # Attention\n    context_vector, attention_weights = BahdanauAttention(hidden_dim, True)(decoder_outputs, encoder_outputs)\n    concat = Concatenate()([decoder_outputs, context_vector])\n\n    outputs = Dense(vocab_size_target, activation='softmax')(concat)\n\n    model = Model([encoder_inputs, decoder_inputs], outputs)\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attention_sweep_config = {\n    'method': 'bayes',\n    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'embedding_dim': {'values': [128, 256]},\n        'hidden_dim': {'values': [128, 256]},\n        'dropout_rate': {'values': [0.0, 0.2, 0.3]},\n        'batch_size': {'values': [32, 64]},\n        'epochs': {'value': 10}\n    }\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sweep_train_attention(config=None):\n    with wandb.init(config=config):\n        config = wandb.config\n        wandb.run.name = f\"attn_emb{config.embedding_dim}_hid{config.hidden_dim}_drop{int(config.dropout_rate*100)}\"\n\n        model = build_attention_seq2seq_model(\n            vocab_size_input=VOCAB_SIZE_INPUT,\n            vocab_size_target=VOCAB_SIZE_TARGET,\n            embedding_dim=config.embedding_dim,\n            hidden_dim=config.hidden_dim,\n            dropout_rate=config.dropout_rate\n        )\n\n        model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n        model.fit(\n            [train_encoder_input, train_decoder_input],\n            train_target_output,\n            validation_data=([val_encoder_input, val_decoder_input], val_target_output),\n            batch_size=config.batch_size,\n            epochs=config.epochs,\n            callbacks=[WandbMetricsLogger()],\n            verbose=2\n        )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sweep_id = wandb.sweep(attention_sweep_config, project=\"DL_A3\")\nwandb.agent(sweep_id, function=sweep_train_attention, count=50)  # Run 10 configs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attention_model = build_attention_seq2seq_model(\n    vocab_size_input=VOCAB_SIZE_INPUT,\n    vocab_size_target=VOCAB_SIZE_TARGET,\n    embedding_dim=256,\n    hidden_dim=256,\n    dropout_rate=0.0\n)\n\nattention_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nattention_model.fit(\n    [train_encoder_input, train_decoder_input],\n    train_target_output,\n    validation_data=([val_encoder_input, val_decoder_input], val_target_output),\n    batch_size=64,\n    epochs=10,\n    verbose=2\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attention_model.save(\"best_attention_model.keras\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nattention_model = load_model('best_attention_model.keras')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.init(project=\"DL_A3\", name='Attention_best_test')\n\n# Evaluate test accuracy\ntest_loss, test_acc = attention_model.evaluate([test_encoder_input, test_decoder_input], test_target_output, verbose=2)\nprint(f\"✅ Test Accuracy (Attention Model): {test_acc:.4f}\")\n\n\nwandb.log({'test_accuracy':test_acc})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(\"predictions_attention\", exist_ok=True)\n\nattention_preds = attention_model.predict([test_encoder_input, test_decoder_input])\nattention_pred_indices = np.argmax(attention_preds, axis=-1)\n\ndecoded_attention_preds = [decode_seq(seq) for seq in attention_pred_indices]\ndecoded_refs = [t.replace(' </s>', '') for t in test_deva_out]\n\nwith open(\"predictions_attention/test_predictions.txt\", \"w\", encoding='utf-8') as f:\n    for inp, pred, ref in zip(test_lat, decoded_attention_preds, decoded_refs):\n        f.write(f\"{inp}\\t{pred}\\t{ref}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vanilla_preds = []\nwith open(\"predictions_vanilla/test_predictions.txt\", encoding=\"utf-8\") as f:\n    for line in f:\n        pred = line.strip().split('\\t')\n        vanilla_preds.append(pred)\n\n# Compare and print improvements\nprint(\"✅ Attention Model Improvements:\")\nfor i, (v, a, r) in enumerate(zip(vanilla_preds, decoded_attention_preds, decoded_refs)):\n    if v != r and a == r:\n        print(f\"{i+1}. Input: {test_lat[i]}\\n   Vanilla: {v}\\n   Attention: {a}\\n   Ref: {r}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoder model for inference\n\"\"\"encoder_model = Model(\n    attention_model.input[0],  # encoder_inputs\n    attention_model.get_layer(\"lstm_3\").output  # encoder_outputs, state_h, state_c\n)\"\"\"\n# Find encoder LSTM\nencoder_lstm = next(layer for layer in attention_model.layers if isinstance(layer, LSTM))\n\n# Get encoder input\nencoder_inputs = attention_model.input[0]  # this is fine\n\n# Get all 3 outputs from encoder LSTM\nencoder_outputs, state_h_enc, state_c_enc = encoder_lstm.output\n\n# Reconstruct encoder model\nencoder_model = Model(encoder_inputs, [encoder_outputs, state_h_enc, state_c_enc])\n\n\n# Decoder setup\ndecoder_inputs = attention_model.input[1]\ndecoder_emb_layer = attention_model.layers[3]\ndecoder_lstm_layer = attention_model.layers[4]\nattention_layer = attention_model.layers[5]\nconcat_layer = attention_model.layers[6]\noutput_layer = attention_model.layers[7]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decode_with_attention(input_seq, max_len=MAXLEN_TARGET):\n    encoder_outs, state_h, state_c = encoder_model.predict(input_seq)\n    \n    target_seq = np.zeros((1, 1))\n    target_seq[0, 0] = target_tokenizer.word_index['\\t']\n\n    decoded = []\n    attention_weights_all = []\n\n    for _ in range(max_len):\n        dec_emb = decoder_emb_layer(target_seq)\n        print(\"dec_emb shape:\", dec_emb.shape)\n        print(\"state_h shape:\", state_h.shape)\n        print(\"state_c shape:\", state_c.shape)\n\n        dec_out, h, c = decoder_lstm_layer(dec_emb, initial_state=[state_h, state_c])\n        context_vector, attn_weights = attention_layer(dec_out, encoder_outs)\n        concat = concat_layer([dec_out, context_vector])\n        output_probs = output_layer(concat)\n        sampled_token = np.argmax(output_probs[0, -1, :])\n        decoded.append(sampled_token)\n        attention_weights_all.append(attn_weights.numpy()[0][0])  # shape: (enc_len,)\n\n        if sampled_token == target_tokenizer.word_index['\\n']:\n            break\n\n        target_seq[0, 0] = sampled_token\n        state_h, state_c = h, c\n\n    decoded_text = ''.join([index_to_char.get(idx, '') for idx in decoded])\n    return decoded_text, attention_weights_all\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_attention_heatmap(attn, input_text, output_text, idx=1):\n    plt.figure(figsize=(6, 5))\n    ax = sns.heatmap(\n        attn,\n        xticklabels=list(input_text),\n        yticklabels=list(output_text),\n        cmap='magma',\n        cbar=False,\n        linewidths=0.5,\n        annot=False\n    )\n    plt.xlabel(\"Input (Latin)\")\n    plt.ylabel(\"Output (Hindi)\")\n    plt.title(f\"Sample {idx} Attention\")\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_9_grid():\n    plt.figure(figsize=(18, 15))\n    for i in range(9):\n        input_text = test_lat[i]\n        input_seq = test_encoder_input[i:i+1]\n        output_text, attn_weights = decode_with_attention(input_seq)\n        attn_matrix = np.stack(attn_weights)  # shape: (dec_len, enc_len)\n\n        plt.subplot(3, 3, i+1)\n        sns.heatmap(attn_matrix, xticklabels=list(input_text), yticklabels=list(output_text), cmap='coolwarm', cbar=False)\n        plt.title(f\"Input: {input_text}\")\n        plt.xlabel(\"Latin chars\")\n        plt.ylabel(\"Hindi chars\")\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(\"attention_heatmaps\", exist_ok=True)\nfor i in range(10):\n    input_text = test_lat[i]\n    input_seq = test_encoder_input[i:i+1]\n    output_text, attn_weights = decode_with_attention(input_seq)\n    attn_matrix = np.stack(attn_weights)\n\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(attn_matrix, xticklabels=list(input_text), yticklabels=list(output_text), cmap='plasma')\n    plt.title(f\"Sample {i+1}\")\n    plt.xlabel(\"Input (Latin)\")\n    plt.ylabel(\"Output (Hindi)\")\n    plt.tight_layout()\n    plt.savefig(f\"attention_heatmaps/sample_{i+1}.png\")\n    plt.close()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question 6 - Connectivity visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef plot_attention_heatmap(input_text, output_text, attention_weights):\n    \"\"\"\n    input_text: str (e.g., 'ghar')\n    output_text: str (e.g., 'घर')\n    attention_weights: list of attention vectors per output token.\n                       Shape: (output_length, input_length)\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(min(10, len(input_text)), min(6, len(output_text))))\n    \n    sns.heatmap(\n        np.array(attention_weights),\n        xticklabels=list(input_text),\n        yticklabels=list(output_text),\n        cmap='Greens',\n        cbar=True,\n        linewidths=0.5,\n        linecolor='gray',\n        ax=ax\n    )\n    \n    ax.set_xlabel(\"Input\")\n    ax.set_ylabel(\"Output\")\n    ax.set_title(\"Attention Heatmap\")\n    plt.yticks(rotation=0)\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}