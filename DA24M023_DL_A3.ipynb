{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DA6401 Assignment 3 - Transliteration model (Encoder/Decoder model)\n",
    "## Sudhanva Satish - DA24M023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Imports & Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, SimpleRNN, LSTM, GRU\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "wandb.login(key=\"<Add your key>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. Load Dakshina Dataset\n",
    "DATA_DIR = \"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/hi/lexicons\"\n",
    "\n",
    "def load_tsv(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "    return [line.split('\\t') for line in lines if '\\t' in line]\n",
    "\n",
    "train_pairs = load_tsv(os.path.join(DATA_DIR, \"hi.translit.sampled.train.tsv\"))\n",
    "val_pairs = load_tsv(os.path.join(DATA_DIR, \"hi.translit.sampled.dev.tsv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"# 2. Load Dakshina Dataset\n",
    "DATA_DIR = \"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/ka/lexicons\"\n",
    "\n",
    "def load_tsv(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "    return [line.split('\\t') for line in lines if '\\t' in line]\n",
    "\n",
    "train_pairs = load_tsv(os.path.join(DATA_DIR, \"ka.translit.sampled.train.tsv\"))\n",
    "val_pairs = load_tsv(os.path.join(DATA_DIR, \"ka.translit.sampled.dev.tsv\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3. Tokenization\n",
    "def tokenize_pairs(pairs):\n",
    "    latin_texts = [x[1] for x in pairs]         # What user types (input)\n",
    "    devanagari_texts = [x[0] for x in pairs]    # What model should output\n",
    "\n",
    "    devanagari_texts_in  = ['\\t' + t for t in devanagari_texts]\n",
    "    devanagari_texts_out = [t + '\\n' for t in devanagari_texts]\n",
    "    return latin_texts, devanagari_texts_in, devanagari_texts_out\n",
    "\n",
    "\n",
    "train_lat, train_deva_in, train_deva_out = tokenize_pairs(train_pairs)\n",
    "val_lat, val_deva_in, val_deva_out = tokenize_pairs(val_pairs)\n",
    "\n",
    "def fit_char_tokenizer(texts):\n",
    "    tokenizer = Tokenizer(char_level=True, lower=False)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "input_tokenizer = fit_char_tokenizer(train_lat + val_lat)\n",
    "target_tokenizer = fit_char_tokenizer(train_deva_in + train_deva_out)\n",
    "\n",
    "VOCAB_SIZE_INPUT = len(input_tokenizer.word_index) + 1\n",
    "VOCAB_SIZE_TARGET = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "def encode_and_pad(texts, tokenizer, maxlen=None):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), padding='post', maxlen=maxlen)\n",
    "\n",
    "MAXLEN_INPUT = max(map(len, train_lat))\n",
    "MAXLEN_TARGET = max(map(len, train_deva_out))\n",
    "\n",
    "train_encoder_input = encode_and_pad(train_lat, input_tokenizer, MAXLEN_INPUT)\n",
    "train_decoder_input = encode_and_pad(train_deva_in, target_tokenizer, MAXLEN_TARGET)\n",
    "train_target_output = np.expand_dims(encode_and_pad(train_deva_out, target_tokenizer, MAXLEN_TARGET), -1)\n",
    "\n",
    "val_encoder_input = encode_and_pad(val_lat, input_tokenizer, MAXLEN_INPUT)\n",
    "val_decoder_input = encode_and_pad(val_deva_in, target_tokenizer, MAXLEN_TARGET)\n",
    "val_target_output = np.expand_dims(encode_and_pad(val_deva_out, target_tokenizer, MAXLEN_TARGET), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train_lat[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Error checking\n",
    "\"\"\"start_token = target_tokenizer.word_index.get('<s>')\n",
    "end_token = target_tokenizer.word_index.get('</s>')\n",
    "\n",
    "if start_token is None or end_token is None:\n",
    "    raise ValueError(\"Start/end tokens not found in tokenizer vocabulary. Did you include <s> and </s> during training?\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4. Build Seq2Seq Model\n",
    "def build_seq2seq_model(vocab_size, embedding_dim, hidden_dim, cell_type, num_encoder_layers, num_decoder_layers, dropout_rate):\n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    embedding = Embedding(vocab_size, embedding_dim, mask_zero=True, name=\"embedding\")\n",
    "    enc_emb = embedding(encoder_inputs)\n",
    "    dec_emb = embedding(decoder_inputs)\n",
    "    RNN = {\"RNN\": SimpleRNN, \"LSTM\": LSTM}[cell_type]\n",
    "\n",
    "    x = enc_emb\n",
    "    for i in range(num_encoder_layers):\n",
    "        rnn = RNN(hidden_dim, return_sequences=True, return_state=True, dropout=dropout_rate, name=f\"encoder_{cell_type}_{i}\")\n",
    "        if cell_type == \"LSTM\":\n",
    "            x, state_h, state_c = rnn(x)\n",
    "            encoder_states = [state_h, state_c]\n",
    "        else:\n",
    "            x, state_h = rnn(x)\n",
    "            encoder_states = [state_h]\n",
    "\n",
    "    y = dec_emb\n",
    "    for i in range(num_decoder_layers):\n",
    "        rnn = RNN(hidden_dim, return_sequences=True, return_state=True, dropout=dropout_rate, name=f\"decoder_{cell_type}_{i}\")\n",
    "        if cell_type == \"LSTM\":\n",
    "            y, _, _ = rnn(y, initial_state=encoder_states)\n",
    "        else:\n",
    "            y, _ = rnn(y, initial_state=encoder_states)\n",
    "\n",
    "    decoder_dense = Dense(vocab_size, activation=\"softmax\", name=\"dense\")\n",
    "    outputs = decoder_dense(y)\n",
    "    return Model([encoder_inputs, decoder_inputs], outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5. Inference Models (LSTM only)\n",
    "def build_inference_models(model, hidden_dim, cell_type):\n",
    "    encoder_inputs = model.input[0]\n",
    "    decoder_inputs = model.input[1]\n",
    "    embedding_layer = model.get_layer(\"embedding\")\n",
    "    encoder_emb = embedding_layer(encoder_inputs)\n",
    "    decoder_emb = embedding_layer(decoder_inputs)\n",
    "\n",
    "    # Dynamically get the first matching RNN layer for encoder and decoder\n",
    "    encoder_rnn = next(layer for layer in model.layers if layer.name.startswith(f\"encoder_{cell_type}\"))\n",
    "    decoder_rnn = next(layer for layer in model.layers if layer.name.startswith(f\"decoder_{cell_type}\"))\n",
    "    decoder_dense = model.get_layer(\"dense\")\n",
    "\n",
    "    if cell_type == \"LSTM\":\n",
    "        _, state_h, state_c = encoder_rnn(encoder_emb)\n",
    "        encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
    "\n",
    "        decoder_state_input_h = Input(shape=(hidden_dim,))\n",
    "        decoder_state_input_c = Input(shape=(hidden_dim,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "        decoder_outputs, state_h, state_c = decoder_rnn(decoder_emb, initial_state=decoder_states_inputs)\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs, state_h, state_c])\n",
    "    else:  # RNN\n",
    "        _, state_h = encoder_rnn(encoder_emb)\n",
    "        encoder_model = Model(encoder_inputs, [state_h])\n",
    "\n",
    "        decoder_state_input_h = Input(shape=(hidden_dim,))\n",
    "        decoder_outputs, state_h = decoder_rnn(decoder_emb, initial_state=[decoder_state_input_h])\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        decoder_model = Model([decoder_inputs, decoder_state_input_h], [decoder_outputs, state_h])\n",
    "\n",
    "    return encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 6. Beam Search Decoder\n",
    "def decode_sequence_beam_search(input_seq, encoder_model, decoder_model, target_tokenizer, beam_width=3, max_output_len=30):\n",
    "    index_to_char = {i: c for c, i in target_tokenizer.word_index.items()}\n",
    "    index_to_char[0] = ''\n",
    "    start_token = target_tokenizer.word_index['<s>']\n",
    "    end_token = target_tokenizer.word_index['</s>']\n",
    "\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    sequences = [([start_token], 0.0, states_value)]\n",
    "\n",
    "    for _ in range(max_output_len):\n",
    "        all_candidates = []\n",
    "        for seq, score, states in sequences:\n",
    "            if seq[-1] == end_token:\n",
    "                all_candidates.append((seq, score, states))\n",
    "                continue\n",
    "            target_seq = np.array([[seq[-1]]])\n",
    "            output_tokens, h, c = decoder_model.predict([target_seq] + states)\n",
    "            top_k = np.argsort(output_tokens[0, -1, :])[-beam_width:]\n",
    "\n",
    "            for token in top_k:\n",
    "                prob = output_tokens[0, -1, token]\n",
    "                candidate = (seq + [token], score - np.log(prob + 1e-9), [h, c])\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        sequences = sorted(all_candidates, key=lambda tup: tup[1])[:beam_width]\n",
    "\n",
    "    best_seq = sequences[0][0]\n",
    "    return ''.join(index_to_char.get(idx, '') for idx in best_seq[1:] if idx != end_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Train the model via wandb sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 7. WandB Sweep Config\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'embedding_dim': {'values': [16, 32, 64, 256]},\n",
    "        'hidden_dim': {'values': [16, 32, 64, 256]},\n",
    "        'cell_type': {'values': ['RNN', 'LSTM']},\n",
    "        'num_encoder_layers': {'values': [1, 2]},\n",
    "        'num_decoder_layers': {'values': [1, 2]},\n",
    "        'dropout_rate': {'values': [0.2, 0.3]},\n",
    "        'batch_size': {'values': [32, 64]},\n",
    "        'epochs': {'value': 10},\n",
    "        'beam_size': {'values': [1, 3, 5]}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 8. Sweep Train Function\n",
    "def sweep_train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        wandb.run.name = f\"{config.cell_type}_emb{config.embedding_dim}_hid{config.hidden_dim}_enc{config.num_encoder_layers}_dec{config.num_decoder_layers}_drop{int(config.dropout_rate*100)}_beam{config.beam_size}\"\n",
    "\n",
    "        model = build_seq2seq_model(\n",
    "            vocab_size=VOCAB_SIZE_TARGET,\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            cell_type=config.cell_type,\n",
    "            num_encoder_layers=config.num_encoder_layers,\n",
    "            num_decoder_layers=config.num_decoder_layers,\n",
    "            dropout_rate=config.dropout_rate\n",
    "        )\n",
    "\n",
    "        model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        model.fit(\n",
    "            [train_encoder_input, train_decoder_input],\n",
    "            train_target_output,\n",
    "            validation_data=([val_encoder_input, val_decoder_input], val_target_output),\n",
    "            batch_size=config.batch_size,\n",
    "            epochs=config.epochs,\n",
    "            callbacks=[WandbMetricsLogger(), WandbModelCheckpoint(\"model_checkpoint.keras\")],\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        # Beam Search part - commented to reduce runtime\n",
    "        \"\"\"encoder_model, decoder_model = build_inference_models(model, config.hidden_dim, config.cell_type)\n",
    "        input_seq = val_encoder_input[0:1]\n",
    "        prediction = decode_sequence_beam_search(\n",
    "            input_seq=input_seq,\n",
    "            encoder_model=encoder_model,\n",
    "            decoder_model=decoder_model,\n",
    "            target_tokenizer=target_tokenizer,\n",
    "            beam_width=config.beam_size\n",
    "        )\n",
    "\n",
    "        print(\"Predicted:\", prediction)\n",
    "        print(\"Ground truth:\", val_deva_out[0])\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 9. Start the Sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"DL_A3\")\n",
    "wandb.agent(sweep_id, function=sweep_train, count=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 in report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 - Evaluate model on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_config = {\n",
    "    'embedding_dim': 256,\n",
    "    'hidden_dim': 256,\n",
    "    'cell_type': 'LSTM',\n",
    "    'num_encoder_layers': 2,\n",
    "    'num_decoder_layers': 2,\n",
    "    'dropout_rate': 0.2,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 10#20\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_model = build_seq2seq_model(\n",
    "    vocab_size=VOCAB_SIZE_TARGET,\n",
    "    embedding_dim=best_config['embedding_dim'],\n",
    "    hidden_dim=best_config['hidden_dim'],\n",
    "    cell_type=best_config['cell_type'],\n",
    "    num_encoder_layers=best_config['num_encoder_layers'],\n",
    "    num_decoder_layers=best_config['num_decoder_layers'],\n",
    "    dropout_rate=best_config['dropout_rate']\n",
    ")\n",
    "\n",
    "best_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "best_model.fit(\n",
    "    [train_encoder_input, train_decoder_input],\n",
    "    train_target_output,\n",
    "    validation_data=([val_encoder_input, val_decoder_input], val_target_output),\n",
    "    batch_size=best_config['batch_size'],\n",
    "    epochs=best_config['epochs'],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "best_model.save(\"best_model.keras\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "best_model = load_model(\"best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_path = \"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n",
    "#test_path = \"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/ka/lexicons/ka.translit.sampled.test.tsv\"\n",
    "test_pairs = load_tsv(test_path)\n",
    "test_lat, test_deva_in, test_deva_out = tokenize_pairs(test_pairs)\n",
    "\n",
    "test_encoder_input = encode_and_pad(test_lat, input_tokenizer, MAXLEN_INPUT)\n",
    "test_decoder_input = encode_and_pad(test_deva_in, target_tokenizer, MAXLEN_TARGET)\n",
    "test_target_output = encode_and_pad(test_deva_out, target_tokenizer, MAXLEN_TARGET)\n",
    "test_target_output = np.expand_dims(test_target_output, -1)\n",
    "\n",
    "wandb.init(project=\"DL_A3\", name=\"Vanilla_best_test\")\n",
    "\n",
    "test_loss, test_acc = best_model.evaluate([test_encoder_input, test_decoder_input], test_target_output)\n",
    "print(f\"✅ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "wandb.log({'test_accuracy':test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "index_to_char = {i: c for c, i in target_tokenizer.word_index.items()}\n",
    "index_to_char[0] = ''\n",
    "\n",
    "def decode_seq(seq):\n",
    "    decoded = []\n",
    "    for idx in seq:\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        token = index_to_char.get(idx, '')  # fallback to '' if invalid index\n",
    "        # check this token based on what was given first\n",
    "        if token in ['\\n']:\n",
    "            break\n",
    "        decoded.append(token)\n",
    "    return ''.join(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"predictions_vanilla\", exist_ok=True)\n",
    "preds = best_model.predict([test_encoder_input, test_decoder_input])\n",
    "pred_indices = np.argmax(preds, axis=-1)\n",
    "\n",
    "index_to_char = {i: c for c, i in target_tokenizer.word_index.items()}\n",
    "index_to_char[0] = ''\n",
    "\n",
    "def decode_seq(seq):\n",
    "    decoded = []\n",
    "    for idx in seq:\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        token = index_to_char.get(idx, '')  # fallback to '' if invalid index\n",
    "        if token in ['\\n']:#['<','\\\\','/']:\n",
    "            break\n",
    "        decoded.append(token)\n",
    "    return ''.join(decoded)\n",
    "\n",
    "\n",
    "decoded_preds = [decode_seq(seq) for seq in pred_indices]\n",
    "# Here too\n",
    "decoded_refs = [x.replace(' </s>', '') for x in test_deva_out]\n",
    "\n",
    "with open(\"predictions_vanilla/test_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for inp, pred, ref in zip(test_lat, decoded_preds, decoded_refs):\n",
    "        f.write(f\"{inp}\\t{pred}\\t{ref}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display first 10 test predictions\n",
    "for i in range(10):\n",
    "    input_latin = test_lat[i]\n",
    "    predicted = decoded_preds[i]\n",
    "    reference = decoded_refs[i]\n",
    "    print(f\"{i+1}. Input: {input_latin}\\n   Predicted: {predicted}\\n   Reference: {reference}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize a Wandb run\n",
    "wandb.init(project=\"seq2seq_sweep\", name=\"prediction_samples_colored_table_10\")\n",
    "\n",
    "# Create a Wandb table\n",
    "table = wandb.Table(columns=[\"Input Word\", \"Predicted Word\", \"Target Word\"])\n",
    "\n",
    "# Sample predictions (10 rows)\n",
    "samples = [\n",
    "    (\"ankit\", \"अंकीत\", \"अंकित\"),\n",
    "    (\"angreji\", \"अगग्ेजी\", \"अंग्रज़ी\"),\n",
    "    (\"andhapan\", \"अंधापन\", \"अंधापन\"),\n",
    "    (\"achnera\", \"अच्रेर\", \"अछनेरा\"),\n",
    "    (\"advait\", \"एडववि\", \"अद्वैत\"),\n",
    "    (\"aakar\", \"आकार\", \"आकार\"),\n",
    "    (\"anupam\", \"अनुपम\", \"अनुपम\"),\n",
    "    (\"aadesh\", \"आदेश\", \"आदेश\"),\n",
    "    (\"abhay\", \"अभाय\", \"अभय\"),\n",
    "    (\"aastik\", \"आस्तिक्\", \"आस्तिक\")\n",
    "]\n",
    "\n",
    "# Add data with color-coded Predicted Word\n",
    "for input_word, pred_word, target_word in samples:\n",
    "    # Determine color based on exact match\n",
    "    color = \"#00FF00\" if pred_word == target_word else \"#FF0000\"\n",
    "    # Wrap predicted word in HTML span\n",
    "    colored_pred = f'<span style=\"color: {color}\">{pred_word}</span>'\n",
    "    table.add_data(input_word, colored_pred, target_word)\n",
    "\n",
    "# Log the table to the Wandb run\n",
    "wandb.log({\"Prediction Samples Colored Table\": table})\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 - Add Attention mechanism to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "attention_config = {\n",
    "    'embedding_dim': 256,\n",
    "    'hidden_dim': 256,\n",
    "    'dropout_rate': 0.2,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 10\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Attention, Concatenate\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query: (batch_size, dec_len, hidden)\n",
    "        # values: (batch_size, enc_len, hidden)\n",
    "        query_with_time_axis = tf.expand_dims(query, 2)  # (batch_size, dec_len, 1, hidden)\n",
    "        values_with_time_axis = tf.expand_dims(values, 1)  # (batch_size, 1, enc_len, hidden)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(self.W1(values_with_time_axis) + self.W2(query_with_time_axis)))  # (batch_size, dec_len, enc_len, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=2)  # (batch_size, dec_len, enc_len, 1)\n",
    "        context_vector = attention_weights * values_with_time_axis  # (batch_size, dec_len, enc_len, hidden)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=2)  # (batch_size, dec_len, hidden)\n",
    "        return context_vector, tf.squeeze(attention_weights, -1)  # return both context and weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Modified attention class to export weights\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units, return_attention=False):\n",
    "        super().__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 2)  # (batch, dec_len, 1, hidden)\n",
    "        values_with_time_axis = tf.expand_dims(values, 1)  # (batch, 1, enc_len, hidden)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(self.W1(values_with_time_axis) + self.W2(query_with_time_axis)))  # (batch, dec_len, enc_len, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=2)\n",
    "        context_vector = tf.reduce_sum(attention_weights * values_with_time_axis, axis=2)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return context_vector, tf.squeeze(attention_weights, -1)\n",
    "        return context_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_attention_seq2seq_model(vocab_size_input, vocab_size_target, embedding_dim, hidden_dim, dropout_rate):\n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    encoder_emb = Embedding(vocab_size_input, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "    encoder_outputs, state_h, state_c = LSTM(hidden_dim, return_sequences=True, return_state=True)(encoder_emb)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_emb = Embedding(vocab_size_target, embedding_dim, mask_zero=True)(decoder_inputs)\n",
    "    decoder_outputs, _, _ = LSTM(hidden_dim, return_sequences=True, return_state=True)(decoder_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "    # Attention\n",
    "    context_vector, attention_weights = BahdanauAttention(hidden_dim)(decoder_outputs, encoder_outputs)\n",
    "    concat = Concatenate()([decoder_outputs, context_vector])\n",
    "\n",
    "    outputs = Dense(vocab_size_target, activation='softmax')(concat)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Attention extraction model\n",
    "\n",
    "def build_attention_seq2seq_model(vocab_size_input, vocab_size_target, embedding_dim, hidden_dim, dropout_rate):\n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    encoder_emb = Embedding(vocab_size_input, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "    encoder_outputs, state_h, state_c = LSTM(hidden_dim, return_sequences=True, return_state=True)(encoder_emb)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_emb = Embedding(vocab_size_target, embedding_dim, mask_zero=True)(decoder_inputs)\n",
    "    decoder_outputs, _, _ = LSTM(hidden_dim, return_sequences=True, return_state=True)(decoder_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "    # Attention\n",
    "    context_vector, attention_weights = BahdanauAttention(hidden_dim, True)(decoder_outputs, encoder_outputs)\n",
    "    concat = Concatenate()([decoder_outputs, context_vector])\n",
    "\n",
    "    outputs = Dense(vocab_size_target, activation='softmax')(concat)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "attention_sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'embedding_dim': {'values': [128, 256]},\n",
    "        'hidden_dim': {'values': [128, 256]},\n",
    "        'dropout_rate': {'values': [0.0, 0.2, 0.3]},\n",
    "        'batch_size': {'values': [32, 64]},\n",
    "        'epochs': {'value': 10}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sweep_train_attention(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        wandb.run.name = f\"attn_emb{config.embedding_dim}_hid{config.hidden_dim}_drop{int(config.dropout_rate*100)}\"\n",
    "\n",
    "        model = build_attention_seq2seq_model(\n",
    "            vocab_size_input=VOCAB_SIZE_INPUT,\n",
    "            vocab_size_target=VOCAB_SIZE_TARGET,\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            dropout_rate=config.dropout_rate\n",
    "        )\n",
    "\n",
    "        model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        model.fit(\n",
    "            [train_encoder_input, train_decoder_input],\n",
    "            train_target_output,\n",
    "            validation_data=([val_encoder_input, val_decoder_input], val_target_output),\n",
    "            batch_size=config.batch_size,\n",
    "            epochs=config.epochs,\n",
    "            callbacks=[WandbMetricsLogger()],\n",
    "            verbose=2\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(attention_sweep_config, project=\"DL_A3\")\n",
    "wandb.agent(sweep_id, function=sweep_train_attention, count=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "attention_model = build_attention_seq2seq_model(\n",
    "    vocab_size_input=VOCAB_SIZE_INPUT,\n",
    "    vocab_size_target=VOCAB_SIZE_TARGET,\n",
    "    embedding_dim=256,\n",
    "    hidden_dim=256,\n",
    "    dropout_rate=0.0\n",
    ")\n",
    "\n",
    "attention_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "attention_model.fit(\n",
    "    [train_encoder_input, train_decoder_input],\n",
    "    train_target_output,\n",
    "    validation_data=([val_encoder_input, val_decoder_input], val_target_output),\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "attention_model.save(\"best_attention_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "attention_model = load_model('best_attention_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"DL_A3\", name='Attention_best_test')\n",
    "\n",
    "# Evaluate test accuracy\n",
    "test_loss, test_acc = attention_model.evaluate([test_encoder_input, test_decoder_input], test_target_output, verbose=2)\n",
    "print(f\"Test Accuracy (Attention Model): {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "wandb.log({'test_accuracy':test_acc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"predictions_attention\", exist_ok=True)\n",
    "\n",
    "attention_preds = attention_model.predict([test_encoder_input, test_decoder_input])\n",
    "attention_pred_indices = np.argmax(attention_preds, axis=-1)\n",
    "\n",
    "decoded_attention_preds = [decode_seq(seq) for seq in attention_pred_indices]\n",
    "decoded_refs = [t.replace(' </s>', '') for t in test_deva_out]\n",
    "\n",
    "with open(\"predictions_attention/test_predictions.txt\", \"w\", encoding='utf-8') as f:\n",
    "    for inp, pred, ref in zip(test_lat, decoded_attention_preds, decoded_refs):\n",
    "        f.write(f\"{inp}\\t{pred}\\t{ref}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vanilla_preds = []\n",
    "with open(\"predictions_vanilla/test_predictions.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        pred = line.strip().split('\\t')\n",
    "        vanilla_preds.append(pred)\n",
    "\n",
    "# Compare and print improvements\n",
    "print(\"Attention Model Improvements:\")\n",
    "for i, (v, a, r) in enumerate(zip(vanilla_preds, decoded_attention_preds, decoded_refs)):\n",
    "    if v != r and a == r:\n",
    "        print(f\"{i+1}. Input: {test_lat[i]}\\n   Vanilla: {v}\\n   Attention: {a}\\n   Ref: {r}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Encoder model for inference\n",
    "\"\"\"encoder_model = Model(\n",
    "    attention_model.input[0],  # encoder_inputs\n",
    "    attention_model.get_layer(\"lstm_3\").output  # encoder_outputs, state_h, state_c\n",
    ")\"\"\"\n",
    "# Find encoder LSTM\n",
    "encoder_lstm = next(layer for layer in attention_model.layers if isinstance(layer, LSTM))\n",
    "\n",
    "# Get encoder input\n",
    "encoder_inputs = attention_model.input[0]  # this is fine\n",
    "\n",
    "# Get all 3 outputs from encoder LSTM\n",
    "encoder_outputs, state_h_enc, state_c_enc = encoder_lstm.output\n",
    "\n",
    "# Reconstruct encoder model\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h_enc, state_c_enc])\n",
    "\n",
    "\n",
    "# Decoder setup\n",
    "decoder_inputs = attention_model.input[1]\n",
    "decoder_emb_layer = attention_model.layers[3]\n",
    "decoder_lstm_layer = attention_model.layers[4]\n",
    "attention_layer = attention_model.layers[5]\n",
    "concat_layer = attention_model.layers[6]\n",
    "output_layer = attention_model.layers[7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def decode_with_attention(input_seq, max_len=MAXLEN_TARGET):\n",
    "    encoder_outs, state_h, state_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_tokenizer.word_index['\\t']\n",
    "\n",
    "    decoded = []\n",
    "    attention_weights_all = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        dec_emb = decoder_emb_layer(target_seq)\n",
    "        print(\"dec_emb shape:\", dec_emb.shape)\n",
    "        print(\"state_h shape:\", state_h.shape)\n",
    "        print(\"state_c shape:\", state_c.shape)\n",
    "\n",
    "        dec_out, h, c = decoder_lstm_layer(dec_emb, initial_state=[state_h, state_c])\n",
    "        context_vector, attn_weights = attention_layer(dec_out, encoder_outs)\n",
    "        concat = concat_layer([dec_out, context_vector])\n",
    "        output_probs = output_layer(concat)\n",
    "        sampled_token = np.argmax(output_probs[0, -1, :])\n",
    "        decoded.append(sampled_token)\n",
    "        attention_weights_all.append(attn_weights.numpy()[0][0])  # shape: (enc_len,)\n",
    "\n",
    "        if sampled_token == target_tokenizer.word_index['\\n']:\n",
    "            break\n",
    "\n",
    "        target_seq[0, 0] = sampled_token\n",
    "        state_h, state_c = h, c\n",
    "\n",
    "    decoded_text = ''.join([index_to_char.get(idx, '') for idx in decoded])\n",
    "    return decoded_text, attention_weights_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_attention_heatmap(attn, input_text, output_text, idx=1):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    ax = sns.heatmap(\n",
    "        attn,\n",
    "        xticklabels=list(input_text),\n",
    "        yticklabels=list(output_text),\n",
    "        cmap='magma',\n",
    "        cbar=False,\n",
    "        linewidths=0.5,\n",
    "        annot=False\n",
    "    )\n",
    "    plt.xlabel(\"Input (Latin)\")\n",
    "    plt.ylabel(\"Output (Hindi)\")\n",
    "    plt.title(f\"Sample {idx} Attention\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_9_grid():\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    for i in range(9):\n",
    "        input_text = test_lat[i]\n",
    "        input_seq = test_encoder_input[i:i+1]\n",
    "        output_text, attn_weights = decode_with_attention(input_seq)\n",
    "        attn_matrix = np.stack(attn_weights)  # shape: (dec_len, enc_len)\n",
    "\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        sns.heatmap(attn_matrix, xticklabels=list(input_text), yticklabels=list(output_text), cmap='coolwarm', cbar=False)\n",
    "        plt.title(f\"Input: {input_text}\")\n",
    "        plt.xlabel(\"Latin chars\")\n",
    "        plt.ylabel(\"Hindi chars\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"attention_heatmaps\", exist_ok=True)\n",
    "for i in range(10):\n",
    "    input_text = test_lat[i]\n",
    "    input_seq = test_encoder_input[i:i+1]\n",
    "    output_text, attn_weights = decode_with_attention(input_seq)\n",
    "    attn_matrix = np.stack(attn_weights)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(attn_matrix, xticklabels=list(input_text), yticklabels=list(output_text), cmap='plasma')\n",
    "    plt.title(f\"Sample {i+1}\")\n",
    "    plt.xlabel(\"Input (Latin)\")\n",
    "    plt.ylabel(\"Output (Hindi)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"attention_heatmaps/sample_{i+1}.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6 - Connectivity visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention_heatmap(input_text, output_text, attention_weights):\n",
    "    \"\"\"\n",
    "    input_text: str (e.g., 'ghar')\n",
    "    output_text: str (e.g., 'घर')\n",
    "    attention_weights: list of attention vectors per output token.\n",
    "                       Shape: (output_length, input_length)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(min(10, len(input_text)), min(6, len(output_text))))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        np.array(attention_weights),\n",
    "        xticklabels=list(input_text),\n",
    "        yticklabels=list(output_text),\n",
    "        cmap='Greens',\n",
    "        cbar=True,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray',\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel(\"Input\")\n",
    "    ax.set_ylabel(\"Output\")\n",
    "    ax.set_title(\"Attention Heatmap\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_heatmap(\n",
    "    input_text=\"ghar\",\n",
    "    output_text=\"घर\",\n",
    "    attention_weights=attn_weights)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7386471,
     "sourceId": 11765869,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
